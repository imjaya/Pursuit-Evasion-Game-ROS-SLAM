# Pursuit-Evasion-Game-ROS-SLAM

# Task 1:
In this task we ROS’s internal Gmapping sever client architecture to
use the LIDAR and the cameras of the turtlebot to map the surrounding
and create a map.
We use the provided launch file to launch the gazebo node with a
certain world index and to launch the gmapping node.
We then launch the turtlebot teleop node provided in the turtlebot
package to move the turtlebot around the house to form its map.
This map can be visualised in rviz. We finally save this map using the
map server.

# Task 2:
In this task we use the provided launch file to launch one the maps
formed in task 1. We give the path to the ‘.yaml’ file of the map.
This launch file also launched the SLAM node of the turtlebot and opens
the map image file in rviz. We add a goal section in rviz and assign the
topic ‘/move_base_simple’ to it and using the ‘2D goal’ option in the
rviz gui, we publish the goal location to the turtlebot.
ROS internally uses global and local planner to move the turtlebot to
the desired goal location and orientation. The global planner plans the
shortest path from initial location to final location and the local planner
plan intermediate paths along multiple points in the global planned
path in order to avoid obstacles in the map.

# Task 3:
In this task we create a tracking node. We do this in three steps:
o Getting the image feed
o Object detection
o Publishing driving instructions to follow the object
To get the image feed we simply create a subscriber node to subscribe
to the ‘tb_1/camera/rgb/image_raw’ topic and read the ‘Image’
message. Since the namespaces are being used we use tb_1 for
pursuer.
For object detection we use Haar Cascade method with HOG
(Histogram of Gradients) filters. This method takes in the raw rgb
image from the turtlebot camera and draws a bounding box around the
human if detected.
The HOG filters are generated by detecting multiple gradients across
the image and the Haar Cascade algorithm assigns weights for those
filters. These filters are multiplied with the entire image and the coordinates
of the bounding box are obtained.
After the bounding box is detected another method takes the centroid
of the bounding box and calculates the distance between this point and
the center of the image. Then it determines the amount the turtlebot
needs to rotate based on this distance.

# Task 4:
1) detector.py is the file which tracks the human model . The
file creates a node and publishes the bounding box to the topic
‘roi’ along with the x,y coordinates of the centroid
2) task4.py is the file which does the pursuing of the human
model. The node in task4 subscribes to the depth camera as
well as the roi topic for the depth and centroid of the bounding
box respectively.
3)The cmd_vel topic is used to move the bot towards the
human model. The angular velocity of the bot is given with a
certain threshold which is the function of the difference
between image centroid and centroid of the bounding box.
4) Lastly the depth is extracted along the same centroid
coordinate of the bounding box and is used for thresholding the
velocity along the x direction
5) We have ran the model 10 times and saw almost 100 %
converges in world index=0. The human model sometimes gets
stuck in the scene,but our bot still ends up following the
human.
